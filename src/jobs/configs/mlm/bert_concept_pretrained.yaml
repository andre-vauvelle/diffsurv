model:
  input_dim: 4697
  output_dim: 794
#  pretrained_embedding_path: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/embeddings/graph_full_211122_prone_32_edge_weights_2021-12-13.pt
  pretrained_embedding_path: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/embeddings/graph_full_211209_prone_256_edge_weights_no_shortcuts_2022-01-05.pt
  embedding_dim: 256
  num_attention_heads: 2
  lr: 0.0001 # from auto lr find
data:
  debug: False
  token_col: concept_id
  label_col: phecode
  token_vocab_path: /SAN/ihibiobank/denaxaslab/andre/UKBB/data/processed/omop/concept_vocab_4697.pkl
  label_vocab_path: /SAN/ihibiobank/denaxaslab/andre/UKBB/data/processed/omop/phecode_vocab_794.pkl
  batch_size: 64
  num_workers: 8
trainer:
  logger:
    - class_path: src.models.loggers.CustomWandbLogger
      init_args:
        project: ehrgnn
        entity: qndre
        save_dir: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/wandb/
        tags:
          - bert
          - concept_id
          - mlm
          - pretrained
        log_model: True
  callbacks:
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        patience: 25
        monitor: hp_metric
  max_epochs: 100
  check_val_every_n_epoch: 1
  val_check_interval: 0.2
  enable_checkpointing: True
  num_sanity_val_steps: 2
  accumulate_grad_batches: 4
