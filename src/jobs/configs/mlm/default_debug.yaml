model:
  input_dim: 4697
  output_dim: 794
  pretrained_embedding_path: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/embeddings/graph_full_211209_prone_256_edge_weights_no_shortcuts_2022-01-05.pt
#  pretrained_embedding_path: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/embeddings/gnn_embeddings_256_1gr128qk_20220217.pt
  freeze_pretrained: False
  embedding_dim: 256
  num_attention_heads: 2
  lr: 0.0001 # from auto lr find
data:
  debug: False
  token_col: concept_id
  label_col: phecode
  token_vocab_path: /SAN/ihibiobank/denaxaslab/andre/UKBB/data/processed/omop/concept_vocab_4697.pkl
  label_vocab_path: /SAN/ihibiobank/denaxaslab/andre/UKBB/data/processed/omop/phecode_vocab_794.pkl
  batch_size: 64
  num_workers: 8
trainer:
#  logger:
#    - class_path: src.models.loggers.CustomWandbLogger
#      init_args:
#        project: diffsurv
#        entity: qndre
#        save_dir: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/wandb/
#        tags:
#          - bert
#          - concept_id
#          - mlm
#          - pretrained
#          - debug
#        log_model: True
  callbacks:
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        patience: 30
        monitor: val/loss
        min_delta: 0.0
        mode: min
  max_epochs: 100
  check_val_every_n_epoch: 1
  val_check_interval: 0.2
  enable_checkpointing: True
  num_sanity_val_steps: 2
  accumulate_grad_batches: 4
  default_root_dir: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/checkpoints/
