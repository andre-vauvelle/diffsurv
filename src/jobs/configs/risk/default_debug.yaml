trainer:
  logger:
    - class_path: pytorch_lightning.loggers.TensorBoardLogger
      init_args:
        name: mlp_debug_phephe
        save_dir: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/tensorboard/
  max_epochs: 20
  check_val_every_n_epoch: 1
  val_check_interval: 0.2
  enable_checkpointing: True
  num_sanity_val_steps: 2
  accumulate_grad_batches: 4
data:
  debug: True
#  pretrained_embedding_path: '/home/vauvelle/pycharm-sftp/ehrgnn/data/graph_full_211122_prone_32_edge_weights_2021-12-13.feather'
  token_col: phecode
  label_col: phecode
  token_vocab_path: /SAN/ihibiobank/denaxaslab/andre/UKBB/data/processed/omop/phecode_vocab_top100_105.pkl
  label_vocab_path: /SAN/ihibiobank/denaxaslab/andre/UKBB/data/processed/omop/phecode_vocab_top100_105.pkl
model:
  input_dim: 105
  output_dim: 105
  count: False

import os

import pandas as pd
import torch
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from torch.utils.data import DataLoader, ConcatDataset
from data.datasets import PheDataset
from model.bert.mlm import BERTMLM
from omni.common import load_pickle
import time

import pytorch_lightning as pl

from definitions import DATA_DIR, MODEL_DIR, MONGO_STR, MONGO_DB, RESULTS_DIR

from sacred import Experiment
from sacred.observers import MongoObserver
from sacred.utils import apply_backspaces_and_linefeeds

base = os.path.basename(__file__)
experiment_name = os.path.splitext(base)[0]
ex = Experiment(experiment_name)
if MONGO_DB is not None:
    ex.observers.append(MongoObserver(url=MONGO_STR, db_name=MONGO_DB))
ex.captured_out_filter = apply_backspaces_and_linefeeds

SYMBOLS = ['PAD',
           'MASK',
           'SEP',
           'CLS',
           'UNK', ]

DEBUG = __debug__
# DEBUG = True
# DEBUG = False
# DEBUG_STRING = 'debug' if __debug__ else ''
DEBUG_STRING = ''


@ex.config
def config():
    name = 'default'
    global_params = {
        'use_code': 'code',  # 'phecode'
        'with_codes': 'all',
        'max_len_seq': 256,
    }

    file_config = {
        'phe_vocab': os.path.join(DATA_DIR, 'processed', global_params['with_codes'], 'phecode_vocab.pkl'),
        # vocabulary idx2token, token2idx
        'code_vocab': os.path.join(DATA_DIR, 'processed', global_params['with_codes'], 'code_vocab.pkl'),
        # vocabulary idx2token, token2idx
        'age_vocab': os.path.join(DATA_DIR, 'processed', global_params['with_codes'], 'age_vocab.pkl'),
        # vocabulary idx2token, token2idx
        'train_data': os.path.join(DATA_DIR, 'processed', global_params['with_codes'], 'MLM', 'phe_train.parquet'),
        'val_data': os.path.join(DATA_DIR, 'processed', global_params['with_codes'], 'MLM', 'phe_val.parquet'),
        'test_data': os.path.join(DATA_DIR, 'processed', global_params['with_codes'], 'MLM', 'phe_test.parquet'),
        # formatted data
    }
    # bert_config = None
    bert_config = {
        'skip_training': False,
        "optim_config": {
            'lr': 1e-4,
            'warmup_proportion': 0.1,
            'weight_decay': 0.01
        },
        "train_params": {
            'epochs': 1 if DEBUG else 100,
            'batch_size': 64,
            'accumulate_grad_batches': 4,
            'effective_batch_size': 256,
            'gpus': -1 if torch.cuda.is_available() else 0,
            'auto_scale_batch_size': False,
            'auto_lr_find': False,
            'val_check_interval': 0.2,
        },
        "model_config": {
            'hidden_size': 252,  # word embedding and seg embedding hidden size
            'intermediate_size': 507,
            'num_hidden_layers': 8,  # number of multi-head attention layers required
            'num_attention_heads': 12,  # number of attention heads
            # if False then train from scratch, else look in os.path.join(MODEL_DIR, 'lightning')
            'vocab_size': len(load_pickle(file_config['phe_vocab'])['token2idx'].keys()),
            # number of disease + symbols for word embedding
            'seg_vocab_size': 2,  # number of vocab for seg embedding
            'age_vocab_size': None,  # len(load_pickle(file_config['age_vocab'])['token2idx'].keys()),
            # number of vocab for age embedding
            'max_position_embedding': global_params['max_len_seq'],  # maximum number of tokens
            'hidden_dropout_prob': 0.2,  # dropout rate
            'attention_probs_dropout_prob': 0.22,  # multi-head attention dropout rate
            # the size of the "intermediate" layer in the transformer encoder
            'hidden_act': 'gelu',
            # The non-linear activation function in the encoder and the pooler "gelu", 'relu', 'swish' are supported
            'initializer_range': 0.02,  # parameter weight initializer range
        },
        'bert_checkpoint_dir': os.path.join(MODEL_DIR, 'mlm'),
        # 'bert_checkpoint_name': None,
        # 'bert_checkpoint_name': 'mlm-epoch=0-val_loss=0.1265-val_AveragePrecision=1.0000.ckpt',
        'bert_checkpoint_name': 'mlm-epoch=45-val_loss=3.5834-val_AveragePrecision=0.1639val_Precision=0.2089.ckpt',
        'tensorboard_dir': os.path.join(MODEL_DIR, 'tensorboard', 'mlm', name),
        "feature_dict": {
            'age': False,
            'seg': True,
            'position': True,
            'word': True
        },
        "mlm_noise": 0.4,
        'noise_type': 'asymmetric'
    }

    n_workers = 0 if DEBUG else os.cpu_count()

