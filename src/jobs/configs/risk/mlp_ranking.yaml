trainer:
  logger:
#    - class_path: models.loggers.CustomWandbLogger
    - class_path: models.loggers.CustomWandbLogger
      init_args:
        project: diffsurv
        entity: cardiors
        tags:
          - mlp
          - synthetic
          - risk
  callbacks:
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        patience: 100
        monitor: hp_metric
        min_delta: 0.002
        mode: max
        verbose: True
        check_finite: False
#   - class_path: callbacks.OnTrainEndResults
#     init_args:
#       save_dir: "mlp_synthetic"
  max_epochs: 50
  check_val_every_n_epoch: 1
  val_check_interval: 0.2
  enable_checkpointing: False
  num_sanity_val_steps: 0
  accumulate_grad_batches: 1
  default_root_dir: .
#  precision: 16
data:
#  path: /lustre/home/rmhivau/diffsurv/data/synthetic/nonlinear_exp_synthetic_no_censoring.pt
#  path: /lustre/home/rmhivau/diffsurv/data/synthetic/linear_exp_synthetic_no_censoring.pt
  wandb_artifact: cardiors/diffsurv/support.pt:latest
  val_split: 0.2
  batch_size: 64
  num_workers: 1
model:
  head_layers: 2
  embedding_dim: 5
  head_hidden_dim: 1024
  lr: 0.0001 # from auto lr find
  only_covs: True
  cov_size: 3
  hidden_dropout_prob: 0
  batch_norm: True
  loss_str: ranking

#  pretrained_embedding_path: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/embeddings/graph_full_211122_prone_32_edge_weights_2021-12-13_in_gnn.pt
#  embedding_dim: 32

#ckpt_path: /SAN/ihibiobank/denaxaslab/andre/ehrgraphs/models/checkpoints/diffsurv/wwtgf7oi/checkpoints/epoch=3-step=3878.ckpt
